{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1987011 entries, 0 to 1987010\n",
      "Data columns (total 13 columns):\n",
      " #   Column                                      Dtype  \n",
      "---  ------                                      -----  \n",
      " 0   Unnamed: 0                                  int64  \n",
      " 1   label                                       float64\n",
      " 2   date                                        object \n",
      " 3   upvotes                                     float64\n",
      " 4   num_comments                                float64\n",
      " 5   combined_text                               object \n",
      " 6   tokenized_text                              object \n",
      " 7   alphanum_text                               object \n",
      " 8   stopword_removed_text                       object \n",
      " 9   stemmed_text                                object \n",
      " 10  non_stopword_removed_stemmed_text           object \n",
      " 11  combined_stemmed_text                       object \n",
      " 12  combined_non_stopword_removed_stemmed_text  object \n",
      "dtypes: float64(3), int64(1), object(9)\n",
      "memory usage: 197.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/reddit_depression_dataset_cleaned.csv\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>alphanum_text</th>\n",
       "      <th>stopword_removed_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>non_stopword_removed_stemmed_text</th>\n",
       "      <th>combined_stemmed_text</th>\n",
       "      <th>combined_non_stopword_removed_stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-07-14 03:35:09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Deep thoughts underdog Only when we start cons...</td>\n",
       "      <td>['deep', 'thoughts', 'underdog', 'only', 'when...</td>\n",
       "      <td>['deep', 'thoughts', 'underdog', 'only', 'when...</td>\n",
       "      <td>['deep', 'thoughts', 'underdog', 'start', '99'...</td>\n",
       "      <td>['deep', 'thought', 'underdog', 'start', '99',...</td>\n",
       "      <td>['deep', 'thought', 'underdog', 'onli', 'when'...</td>\n",
       "      <td>deep thought underdog start 99 underdog start ...</td>\n",
       "      <td>deep thought underdog onli when we start consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-09-13 00:31:19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I like this sub, there's only two posts yet I ...</td>\n",
       "      <td>['i', 'like', 'this', 'sub', ',', \"there's\", '...</td>\n",
       "      <td>['i', 'like', 'this', 'sub', 'only', 'two', 'p...</td>\n",
       "      <td>['posts', 'coming', 'human', 'morality', 'joke...</td>\n",
       "      <td>['post', 'come', 'human', 'moral', 'joke', 'lo...</td>\n",
       "      <td>['i', 'like', 'thi', 'sub', 'onli', 'two', 'po...</td>\n",
       "      <td>post come human moral joke long abscenc hope d...</td>\n",
       "      <td>i like thi sub onli two post yet i keep come b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-11-20 04:31:58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rebirth! Hello. \\nI am the new guy in charge h...</td>\n",
       "      <td>['rebirth', '!', 'hello', '.', 'i', 'am', 'the...</td>\n",
       "      <td>['rebirth', 'hello', 'i', 'am', 'the', 'new', ...</td>\n",
       "      <td>['rebirth', 'guy', 'charge', 'thegood', 'ofc',...</td>\n",
       "      <td>['rebirth', 'guy', 'charg', 'thegood', 'ofc', ...</td>\n",
       "      <td>['rebirth', 'hello', 'i', 'am', 'the', 'new', ...</td>\n",
       "      <td>rebirth guy charg thegood ofc bring weirdpinea...</td>\n",
       "      <td>rebirth hello i am the new guy in charg here b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-11-20 19:38:05</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>\"I want to be like water. I want to slip throu...</td>\n",
       "      <td>['\"', 'i', 'want', 'to', 'be', 'like', 'water'...</td>\n",
       "      <td>['i', 'want', 'to', 'be', 'like', 'water', 'i'...</td>\n",
       "      <td>['water', 'slip', 'fingers', 'hold', 'ship', '...</td>\n",
       "      <td>['water', 'slip', 'finger', 'hold', 'ship', 'm...</td>\n",
       "      <td>['i', 'want', 'to', 'be', 'like', 'water', 'i'...</td>\n",
       "      <td>water slip finger hold ship michel william</td>\n",
       "      <td>i want to be like water i want to slip through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014-11-22 19:17:39</td>\n",
       "      <td>8.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>What is the limit of the knowledge and power a...</td>\n",
       "      <td>['what', 'is', 'the', 'limit', 'of', 'the', 'k...</td>\n",
       "      <td>['what', 'is', 'the', 'limit', 'of', 'the', 'k...</td>\n",
       "      <td>['limit', 'knowledge', 'power', 'human', 'pers...</td>\n",
       "      <td>['limit', 'knowledg', 'power', 'human', 'perso...</td>\n",
       "      <td>['what', 'is', 'the', 'limit', 'of', 'the', 'k...</td>\n",
       "      <td>limit knowledg power human person infinit grow</td>\n",
       "      <td>what is the limit of the knowledg and power a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                 date  upvotes  num_comments  \\\n",
       "0           0    0.0  2014-07-14 03:35:09      4.0           0.0   \n",
       "1           1    0.0  2014-09-13 00:31:19      4.0           1.0   \n",
       "2           2    0.0  2014-11-20 04:31:58      6.0           1.0   \n",
       "3           3    0.0  2014-11-20 19:38:05     25.0           2.0   \n",
       "4           5    0.0  2014-11-22 19:17:39      8.0          23.0   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Deep thoughts underdog Only when we start cons...   \n",
       "1  I like this sub, there's only two posts yet I ...   \n",
       "2  Rebirth! Hello. \\nI am the new guy in charge h...   \n",
       "3  \"I want to be like water. I want to slip throu...   \n",
       "4  What is the limit of the knowledge and power a...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['deep', 'thoughts', 'underdog', 'only', 'when...   \n",
       "1  ['i', 'like', 'this', 'sub', ',', \"there's\", '...   \n",
       "2  ['rebirth', '!', 'hello', '.', 'i', 'am', 'the...   \n",
       "3  ['\"', 'i', 'want', 'to', 'be', 'like', 'water'...   \n",
       "4  ['what', 'is', 'the', 'limit', 'of', 'the', 'k...   \n",
       "\n",
       "                                       alphanum_text  \\\n",
       "0  ['deep', 'thoughts', 'underdog', 'only', 'when...   \n",
       "1  ['i', 'like', 'this', 'sub', 'only', 'two', 'p...   \n",
       "2  ['rebirth', 'hello', 'i', 'am', 'the', 'new', ...   \n",
       "3  ['i', 'want', 'to', 'be', 'like', 'water', 'i'...   \n",
       "4  ['what', 'is', 'the', 'limit', 'of', 'the', 'k...   \n",
       "\n",
       "                               stopword_removed_text  \\\n",
       "0  ['deep', 'thoughts', 'underdog', 'start', '99'...   \n",
       "1  ['posts', 'coming', 'human', 'morality', 'joke...   \n",
       "2  ['rebirth', 'guy', 'charge', 'thegood', 'ofc',...   \n",
       "3  ['water', 'slip', 'fingers', 'hold', 'ship', '...   \n",
       "4  ['limit', 'knowledge', 'power', 'human', 'pers...   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  ['deep', 'thought', 'underdog', 'start', '99',...   \n",
       "1  ['post', 'come', 'human', 'moral', 'joke', 'lo...   \n",
       "2  ['rebirth', 'guy', 'charg', 'thegood', 'ofc', ...   \n",
       "3  ['water', 'slip', 'finger', 'hold', 'ship', 'm...   \n",
       "4  ['limit', 'knowledg', 'power', 'human', 'perso...   \n",
       "\n",
       "                   non_stopword_removed_stemmed_text  \\\n",
       "0  ['deep', 'thought', 'underdog', 'onli', 'when'...   \n",
       "1  ['i', 'like', 'thi', 'sub', 'onli', 'two', 'po...   \n",
       "2  ['rebirth', 'hello', 'i', 'am', 'the', 'new', ...   \n",
       "3  ['i', 'want', 'to', 'be', 'like', 'water', 'i'...   \n",
       "4  ['what', 'is', 'the', 'limit', 'of', 'the', 'k...   \n",
       "\n",
       "                               combined_stemmed_text  \\\n",
       "0  deep thought underdog start 99 underdog start ...   \n",
       "1  post come human moral joke long abscenc hope d...   \n",
       "2  rebirth guy charg thegood ofc bring weirdpinea...   \n",
       "3         water slip finger hold ship michel william   \n",
       "4     limit knowledg power human person infinit grow   \n",
       "\n",
       "          combined_non_stopword_removed_stemmed_text  \n",
       "0  deep thought underdog onli when we start consi...  \n",
       "1  i like thi sub onli two post yet i keep come b...  \n",
       "2  rebirth hello i am the new guy in charg here b...  \n",
       "3  i want to be like water i want to slip through...  \n",
       "4  what is the limit of the knowledg and power a ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in the text column with an empty string\n",
    "df['combined_non_stopword_removed_stemmed_text'].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(df['combined_non_stopword_removed_stemmed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.82149\teval-auc:0.82203\n",
      "[10]\ttrain-auc:0.88120\teval-auc:0.87986\n",
      "[20]\ttrain-auc:0.89893\teval-auc:0.89686\n",
      "[30]\ttrain-auc:0.91111\teval-auc:0.90855\n",
      "[40]\ttrain-auc:0.92034\teval-auc:0.91774\n",
      "[50]\ttrain-auc:0.92610\teval-auc:0.92325\n",
      "[60]\ttrain-auc:0.93038\teval-auc:0.92747\n",
      "[70]\ttrain-auc:0.93417\teval-auc:0.93118\n",
      "[80]\ttrain-auc:0.93678\teval-auc:0.93363\n",
      "[90]\ttrain-auc:0.93958\teval-auc:0.93637\n",
      "[100]\ttrain-auc:0.94167\teval-auc:0.93839\n",
      "[110]\ttrain-auc:0.94353\teval-auc:0.94022\n",
      "[120]\ttrain-auc:0.94517\teval-auc:0.94178\n",
      "[130]\ttrain-auc:0.94653\teval-auc:0.94308\n",
      "[140]\ttrain-auc:0.94786\teval-auc:0.94431\n",
      "[150]\ttrain-auc:0.94904\teval-auc:0.94538\n",
      "[160]\ttrain-auc:0.95010\teval-auc:0.94638\n",
      "[170]\ttrain-auc:0.95102\teval-auc:0.94727\n",
      "[180]\ttrain-auc:0.95192\teval-auc:0.94812\n",
      "[190]\ttrain-auc:0.95273\teval-auc:0.94886\n",
      "[200]\ttrain-auc:0.95347\teval-auc:0.94956\n",
      "[210]\ttrain-auc:0.95427\teval-auc:0.95027\n",
      "[220]\ttrain-auc:0.95498\teval-auc:0.95094\n",
      "[230]\ttrain-auc:0.95557\teval-auc:0.95151\n",
      "[240]\ttrain-auc:0.95620\teval-auc:0.95208\n",
      "[250]\ttrain-auc:0.95680\teval-auc:0.95261\n",
      "[260]\ttrain-auc:0.95732\teval-auc:0.95308\n",
      "[270]\ttrain-auc:0.95785\teval-auc:0.95356\n",
      "[280]\ttrain-auc:0.95831\teval-auc:0.95399\n",
      "[290]\ttrain-auc:0.95881\teval-auc:0.95441\n",
      "[300]\ttrain-auc:0.95921\teval-auc:0.95475\n",
      "[310]\ttrain-auc:0.95965\teval-auc:0.95514\n",
      "[320]\ttrain-auc:0.96013\teval-auc:0.95556\n",
      "[330]\ttrain-auc:0.96049\teval-auc:0.95588\n",
      "[340]\ttrain-auc:0.96089\teval-auc:0.95623\n",
      "[350]\ttrain-auc:0.96124\teval-auc:0.95657\n",
      "[360]\ttrain-auc:0.96161\teval-auc:0.95690\n",
      "[370]\ttrain-auc:0.96199\teval-auc:0.95722\n",
      "[380]\ttrain-auc:0.96233\teval-auc:0.95750\n",
      "[390]\ttrain-auc:0.96264\teval-auc:0.95778\n",
      "[400]\ttrain-auc:0.96293\teval-auc:0.95801\n",
      "[410]\ttrain-auc:0.96324\teval-auc:0.95828\n",
      "[420]\ttrain-auc:0.96353\teval-auc:0.95853\n",
      "[430]\ttrain-auc:0.96382\teval-auc:0.95878\n",
      "[440]\ttrain-auc:0.96411\teval-auc:0.95904\n",
      "[450]\ttrain-auc:0.96437\teval-auc:0.95925\n",
      "[460]\ttrain-auc:0.96461\teval-auc:0.95945\n",
      "[470]\ttrain-auc:0.96487\teval-auc:0.95966\n",
      "[480]\ttrain-auc:0.96514\teval-auc:0.95986\n",
      "[490]\ttrain-auc:0.96537\teval-auc:0.96006\n",
      "[500]\ttrain-auc:0.96560\teval-auc:0.96024\n",
      "[510]\ttrain-auc:0.96582\teval-auc:0.96044\n",
      "[520]\ttrain-auc:0.96604\teval-auc:0.96062\n",
      "[530]\ttrain-auc:0.96627\teval-auc:0.96078\n",
      "[540]\ttrain-auc:0.96646\teval-auc:0.96095\n",
      "[550]\ttrain-auc:0.96667\teval-auc:0.96113\n",
      "[560]\ttrain-auc:0.96686\teval-auc:0.96129\n",
      "[570]\ttrain-auc:0.96707\teval-auc:0.96145\n",
      "[580]\ttrain-auc:0.96726\teval-auc:0.96161\n",
      "[590]\ttrain-auc:0.96746\teval-auc:0.96178\n",
      "[600]\ttrain-auc:0.96765\teval-auc:0.96195\n",
      "[610]\ttrain-auc:0.96781\teval-auc:0.96207\n",
      "[620]\ttrain-auc:0.96796\teval-auc:0.96218\n",
      "[630]\ttrain-auc:0.96813\teval-auc:0.96231\n",
      "[640]\ttrain-auc:0.96832\teval-auc:0.96248\n",
      "[650]\ttrain-auc:0.96848\teval-auc:0.96260\n",
      "[660]\ttrain-auc:0.96865\teval-auc:0.96275\n",
      "[670]\ttrain-auc:0.96881\teval-auc:0.96288\n",
      "[680]\ttrain-auc:0.96897\teval-auc:0.96301\n",
      "[690]\ttrain-auc:0.96913\teval-auc:0.96315\n",
      "[700]\ttrain-auc:0.96928\teval-auc:0.96328\n",
      "[710]\ttrain-auc:0.96943\teval-auc:0.96338\n",
      "[720]\ttrain-auc:0.96958\teval-auc:0.96349\n",
      "[730]\ttrain-auc:0.96972\teval-auc:0.96360\n",
      "[740]\ttrain-auc:0.96986\teval-auc:0.96370\n",
      "[750]\ttrain-auc:0.97001\teval-auc:0.96382\n",
      "[760]\ttrain-auc:0.97016\teval-auc:0.96392\n",
      "[770]\ttrain-auc:0.97029\teval-auc:0.96401\n",
      "[780]\ttrain-auc:0.97041\teval-auc:0.96409\n",
      "[790]\ttrain-auc:0.97055\teval-auc:0.96419\n",
      "[800]\ttrain-auc:0.97067\teval-auc:0.96431\n",
      "[810]\ttrain-auc:0.97080\teval-auc:0.96441\n",
      "[820]\ttrain-auc:0.97093\teval-auc:0.96451\n",
      "[830]\ttrain-auc:0.97105\teval-auc:0.96459\n",
      "[840]\ttrain-auc:0.97118\teval-auc:0.96469\n",
      "[850]\ttrain-auc:0.97130\teval-auc:0.96478\n",
      "[860]\ttrain-auc:0.97141\teval-auc:0.96488\n",
      "[870]\ttrain-auc:0.97152\teval-auc:0.96495\n",
      "[880]\ttrain-auc:0.97164\teval-auc:0.96504\n",
      "[890]\ttrain-auc:0.97177\teval-auc:0.96513\n",
      "[900]\ttrain-auc:0.97188\teval-auc:0.96522\n",
      "[910]\ttrain-auc:0.97199\teval-auc:0.96531\n",
      "[920]\ttrain-auc:0.97210\teval-auc:0.96539\n",
      "[930]\ttrain-auc:0.97220\teval-auc:0.96546\n",
      "[940]\ttrain-auc:0.97230\teval-auc:0.96553\n",
      "[950]\ttrain-auc:0.97240\teval-auc:0.96560\n",
      "[960]\ttrain-auc:0.97251\teval-auc:0.96568\n",
      "[970]\ttrain-auc:0.97261\teval-auc:0.96577\n",
      "[980]\ttrain-auc:0.97270\teval-auc:0.96583\n",
      "[990]\ttrain-auc:0.97280\teval-auc:0.96590\n",
      "[999]\ttrain-auc:0.97288\teval-auc:0.96596\n"
     ]
    }
   ],
   "source": [
    "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = bst.predict(dtest)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97    352331\n",
      "         1.0       0.84      0.67      0.75     45072\n",
      "\n",
      "    accuracy                           0.95    397403\n",
      "   macro avg       0.90      0.83      0.86    397403\n",
      "weighted avg       0.95      0.95      0.95    397403\n",
      "\n",
      "Confusion Matrix:\n",
      " [[346555   5776]\n",
      " [ 14756  30316]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
